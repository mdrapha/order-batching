{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyspark\n",
      "  Using cached pyspark-3.5.3.tar.gz (317.3 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting py4j==0.10.9.7 (from pyspark)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (pyproject.toml): started\n",
      "  Building wheel for pyspark (pyproject.toml): still running...\n",
      "  Building wheel for pyspark (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840672 sha256=cf531a09a95e701a2f6ff3279523c31f3f47b710054d0c5e87cecdc1e045c4eb\n",
      "  Stored in directory: c:\\users\\davis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local\\pip\\cache\\wheels\\07\\a0\\a3\\d24c94bf043ab5c7e38c30491199a2a11fef8d2584e6df7fb7\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.3\n"
     ]
    }
   ],
   "source": [
    "from produto import Produto\n",
    "from caixa import Caixa\n",
    "from estoque import Estoque\n",
    "from onda import Onda\n",
    "\n",
    "!pip install pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# Configuração inicial do PySpark\n",
    "conf = SparkConf()\n",
    "conf.setAppName(\"MeuAplicativoPySpark\")\n",
    "conf.setMaster(\"local[*]\")  # Usar todos os núcleos locais para execução\n",
    "\n",
    "# Criação da SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession Criada!\n",
      "+-------+--------+-----+-------------+-----+\n",
      "|ONDA_ID|CAIXA_ID|PECAS|  CLASSE_ONDA|  SKU|\n",
      "+-------+--------+-----+-------------+-----+\n",
      "|      4|      12|    1|CLASSE_ONDA_1|SKU_1|\n",
      "+-------+--------+-----+-------------+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----+--------+---------+-----+\n",
      "|ANDAR|CORREDOR|      SKU|PECAS|\n",
      "+-----+--------+---------+-----+\n",
      "|    0|       2|SKU_17028|  193|\n",
      "+-----+--------+---------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verificar se a sessão Spark está funcionando\n",
    "print(\"SparkSession Criada!\")\n",
    "spark.sparkContext.setLogLevel(\"WARN\")  # Define o nível de log para evitar muitas mensagens de depuração\n",
    "\n",
    "# Leitura de dois arquivos CSV e salvando em variáveis\n",
    "caixas_df = spark.read.csv(\"../data/caixas.csv\", header=True, inferSchema=True)\n",
    "estoque_df = spark.read.csv(\"../data/estoque.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Mostrando o conteúdo dos DataFrames\n",
    "caixas_df.show(1)\n",
    "estoque_df.show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma visão temporária para utilizar SQL puro\n",
    "caixas_df.createOrReplaceTempView(\"caixas\")\n",
    "estoque_df.createOrReplaceTempView(\"estoque\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+-----+------+-----------------------+-----------------------+----+\n",
      "|  CLASSE_ONDA|CAIXA_ID|ANDAR|   SKU|TOTAL_PECAS_NECESSARIAS|TOTAL_PECAS_DISPONIVEIS|DIFF|\n",
      "+-------------+--------+-----+------+-----------------------+-----------------------+----+\n",
      "|CLASSE_ONDA_1|      12|    0| SKU_2|                      1|                     84|  83|\n",
      "|CLASSE_ONDA_1|      12|    0| SKU_3|                      1|                    852| 851|\n",
      "|CLASSE_ONDA_1|      12|    0| SKU_4|                      1|                    670| 669|\n",
      "|CLASSE_ONDA_1|      12|    0| SKU_6|                      4|                    233| 229|\n",
      "|CLASSE_ONDA_1|      12|    0| SKU_9|                      4|                    152| 148|\n",
      "|CLASSE_ONDA_1|      12|    0|SKU_11|                      1|                      1|   0|\n",
      "|CLASSE_ONDA_1|      12|    0|SKU_12|                      2|                    132| 130|\n",
      "|CLASSE_ONDA_1|      12|    0|SKU_14|                      8|                    271| 263|\n",
      "|CLASSE_ONDA_1|      12|    0|SKU_15|                      8|                    117| 109|\n",
      "|CLASSE_ONDA_1|      12|    0|SKU_17|                      1|                     64|  63|\n",
      "|CLASSE_ONDA_1|      12|    1| SKU_1|                      1|                      6|   5|\n",
      "|CLASSE_ONDA_1|      12|    1| SKU_5|                      1|                   2667|2666|\n",
      "|CLASSE_ONDA_1|      12|    1|SKU_11|                      1|                    174| 173|\n",
      "|CLASSE_ONDA_1|      12|    2| SKU_1|                      1|                    886| 885|\n",
      "|CLASSE_ONDA_1|      12|    2| SKU_3|                      1|                   1220|1219|\n",
      "|CLASSE_ONDA_1|      12|    2| SKU_7|                      9|                    360| 351|\n",
      "|CLASSE_ONDA_1|      12|    2| SKU_8|                     10|                    435| 425|\n",
      "|CLASSE_ONDA_1|      12|    2| SKU_9|                      4|                    257| 253|\n",
      "|CLASSE_ONDA_1|      12|    2|SKU_10|                      2|                    290| 288|\n",
      "|CLASSE_ONDA_1|      12|    2|SKU_13|                      9|                    203| 194|\n",
      "+-------------+--------+-----+------+-----------------------+-----------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Agregando as quantidades de cada SKU por classe de onda e caixa\n",
    "agregado_caixa = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        CLASSE_ONDA,\n",
    "        CAIXA_ID,\n",
    "        SKU,\n",
    "        PECAS\n",
    "    FROM caixas\n",
    "\n",
    "\"\"\")\n",
    "agregado_caixa.createOrReplaceTempView(\"agregado_caixa\")\n",
    "\n",
    "# Agregando as quantidades de cada SKU por andar\n",
    "agregado_estoque = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        ANDAR,\n",
    "        SKU,\n",
    "        SUM(PECAS) AS TOTAL_PECAS_DISPONIVEIS\n",
    "    FROM estoque\n",
    "    GROUP BY\n",
    "        ANDAR, SKU\n",
    "\"\"\")\n",
    "agregado_estoque.createOrReplaceTempView(\"agregado_estoque\")\n",
    "\n",
    "# Verificando quantas peças são necessárias em cada caixa e quantas temos no andar\n",
    "dados_necessarios_disponiveis = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        ac.CLASSE_ONDA,\n",
    "        ac.CAIXA_ID,\n",
    "        ae.ANDAR,\n",
    "        ac.SKU,\n",
    "        ac.PECAS AS TOTAL_PECAS_NECESSARIAS,\n",
    "        ae.TOTAL_PECAS_DISPONIVEIS,\n",
    "        (ae.TOTAL_PECAS_DISPONIVEIS - ac.PECAS) AS DIFF\n",
    "        \n",
    "    FROM agregado_caixa ac\n",
    "    LEFT JOIN agregado_estoque ae\n",
    "    ON ac.SKU = ae.SKU\n",
    "    ORDER BY\n",
    "        ac.CLASSE_ONDA, ac.CAIXA_ID, ae.ANDAR\n",
    "\"\"\")\n",
    "dados_necessarios_disponiveis.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23299"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtrando os casos onde o DIFF > 0\n",
    "caixas_easy = dados_necessarios_disponiveis.filter(\"DIFF >= 0\")\n",
    "caixas_easy.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caixas_hard = dados_necessarios_disponiveis.filter(\"DIFF < 0\")\n",
    "caixas_hard.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2085\n"
     ]
    }
   ],
   "source": [
    "produtos = {}\n",
    "\n",
    "for row in caixas_df.collect():\n",
    "    sku = row['SKU']\n",
    "    qtd = row['PECAS']\n",
    "    if sku not in produtos:\n",
    "        produtos[sku] = Produto(sku, qtd)\n",
    "    else:\n",
    "        produtos[sku].qtd += qtd \n",
    "\n",
    "print(len(produtos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2072\n"
     ]
    }
   ],
   "source": [
    "caixas = {}\n",
    "for row in caixas_df.collect():\n",
    "    onda_id = row['ONDA_ID']\n",
    "    caixa_id = row['CAIXA_ID']\n",
    "    classe_onda = row['CLASSE_ONDA']\n",
    "    sku = row['SKU']\n",
    "    qtd = row['PECAS']\n",
    "    \n",
    "    # Criar ou buscar a caixa existente\n",
    "    if caixa_id not in caixas:\n",
    "        caixa = Caixa(classe_onda, onda_id)\n",
    "        caixas[caixa_id] = caixa\n",
    "    else:\n",
    "        caixa = caixas[caixa_id]\n",
    "    \n",
    "    # Adicionar o produto à caixa\n",
    "    produto = Produto(sku, qtd)\n",
    "\n",
    "print(len(caixas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7798\n"
     ]
    }
   ],
   "source": [
    "estoques = []\n",
    "for row in estoque_df.collect():\n",
    "    andar = row['ANDAR']\n",
    "    corredor = row['CORREDOR']\n",
    "    sku = row['SKU']\n",
    "    qtd = row['PECAS']\n",
    "    \n",
    "    produto = produtos.get(sku, Produto(sku, 0))\n",
    "    estoque = Estoque(andar, corredor, produto, qtd)\n",
    "    estoques.append(estoque)\n",
    "\n",
    "print(len(estoques))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
